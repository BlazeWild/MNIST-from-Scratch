{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "def load_mnist():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    # Normalize pixel values to between 0 and 1\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    # Flatten images into 1D vectors\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "    # Convert labels to categorical (one-hot encoding)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(60000, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[1975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters (weights and biases)\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "    \n",
    "def relu_derivates(Z): #we need relu derivation for backprop \n",
    "    return (Z>0).astype(int)\n",
    "'''\n",
    "- returns true if Z is greater than 0, ese returns false\n",
    "- converts true and false to 1 and 0\n",
    "'''\n",
    "\n",
    "#softmax, it might give higher numbers as there is e^z\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z/np.sum(exp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_propagation(X,parameters): \n",
    "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "    # Hidden layer\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.relu(Z1)\n",
    "    \n",
    "    # Output layer\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-entropy loss\n",
    "def compute_cost(y, y_hat):\n",
    "    y_hat = y_hat + 1e-15#to avoid log(0)\n",
    "    loss = - np.sum(y*np.log(y_hat))\n",
    "    cost = np.mean(loss)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X,Y,parameters,cache):\n",
    "    m = X_train.shape[0]\n",
    "    A1,A2 = cache['A1'],cache['A2']\n",
    "    W1, W2 = parameters['W1'], parameters['W2']\n",
    "    \n",
    "    dZ2= A2 - Y\n",
    "    dW2 = (1/m)*np.dot(dZ2,A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot(W2.T,dZ2)*relu_derivates(A1)\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
