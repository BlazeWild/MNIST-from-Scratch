{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "def load_mnist():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    # Normalize pixel values to between 0 and 1\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    # Flatten images into 1D vectors\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "    # Convert labels to categorical (one-hot encoding)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(60000, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[1975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters (weights and biases)\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "    \n",
    "def relu_derivates(Z): #we need relu derivation for backprop \n",
    "    return (Z>0).astype(int)\n",
    "'''\n",
    "- returns true if Z is greater than 0, ese returns false\n",
    "- converts true and false to 1 and 0\n",
    "'''\n",
    "\n",
    "#softmax, it might give higher numbers as there is e^z\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z/np.sum(exp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_propagation(X,parameters): \n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    # Hidden layer\n",
    "    Z1 = np.dot(X.T,W1) + b1\n",
    "    A1 = np.relu(Z1)\n",
    "    \n",
    "    # Output layer\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-entropy loss\n",
    "def compute_cost(y, y_hat):\n",
    "    y_hat = y_hat + 1e-15#to avoid log(0)\n",
    "    loss = - np.sum(y*np.log(y_hat))\n",
    "    cost = np.mean(loss)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X,Y,parameters,cache):\n",
    "    m = X_train.shape[0]\n",
    "    A1,A2 = cache['A1'],cache['A2']\n",
    "    W1, W2 = parameters['W1'], parameters['W2']\n",
    "    \n",
    "    dZ2= A2 - Y\n",
    "    dW2 = (1/m)*np.dot(dZ2,A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot(W2.T,dZ2)*relu_derivates(A1)\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(parameters,grads,learning_rate):\n",
    "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "    dW1, db1, dW2, db2 = grads['dW1'], grads['db1'], grads['dW2'], grads['db2']\n",
    "    \n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    return np.mean(y == y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,input_size,hidden_size,output_size,y,learning_rate,epochs):\n",
    "    \n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        y_hat,cache = forward_propagation(X,parameters)\n",
    "        cost = compute_cost(y, y_hat)\n",
    "        grads = backward_propagation(X,y,parameters,cache)\n",
    "        parameters = update_paramters(parameters,grads,learning_rate)\n",
    "        \n",
    "        accuracy = accuracy(y, y_hat)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {i}, cost: {cost}- accuracy: {accuracy}')\n",
    "    return parameters,cost,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):\n",
    "    y_hat,cache = forward_propagation(X,parameters)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parameters,cost,accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X, input_size, hidden_size, output_size, y, learning_rate, epochs)\u001b[0m\n\u001b[0;32m      3\u001b[0m parameters \u001b[38;5;241m=\u001b[39m initialize_parameters(input_size, hidden_size, output_size)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 6\u001b[0m     y_hat,cache \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     cost \u001b[38;5;241m=\u001b[39m compute_cost(y, y_hat)\n\u001b[0;32m      8\u001b[0m     grads \u001b[38;5;241m=\u001b[39m backward_propagation(X,y,parameters,cache)\n",
      "Cell \u001b[1;32mIn[75], line 3\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(X,parameters): \n\u001b[1;32m----> 3\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m\"\u001b[39m], parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m\"\u001b[39m], parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Hidden layer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT,W1) \u001b[38;5;241m+\u001b[39m b1\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "parameters,cost,accuracy = train_model(X_train,784,64,10,y_train,0.01,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
